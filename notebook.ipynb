{"cells":[{"cell_type":"code","execution_count":1,"id":"fd264fe7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.4.0+cu118)\n","Requirement already satisfied: torchvision in c:\\users\\acer\\anaconda3\\lib\\site-packages (0.19.0+cu118)\n","Requirement already satisfied: torchaudio in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.4.0+cu118)\n","Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n","Requirement already satisfied: numpy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":1,"id":"d45d1026-be06-49b5-b6bc-c6dea824a885","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":3902,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1722944758595,"lastExecutedByKernel":"9dc7d9fd-0e7c-4d21-a43a-72c69d455cfa","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# All imports used in this project\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.data.utils import get_tokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch.optim as optim\nfrom torchmetrics import Accuracy, F1Score","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# All imports used in this project\n","import praw\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from torchmetrics import Accuracy, F1Score\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"id":"37eb735b-68d0-42d7-8089-bb68e678fa24","metadata":{"executionCancelledAt":null,"executionTime":21138,"lastExecutedAt":1722944779734,"lastExecutedByKernel":"9dc7d9fd-0e7c-4d21-a43a-72c69d455cfa","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import praw\n\n# Initialize the Reddit client\nreddit = praw.Reddit(\n    client_id='u6EMlVDn7jAwSKDA1A17Ww',\n    client_secret='X2HbDELH7VjM6yOkRv9HXMJf1VcQXQ',\n    user_agent='sentiment_app'\n)\n\n# Creating a list for the subreddit names and for storing the data\nsubreddit_names = ['scarystories', 'angry', 'funnystories', 'sadstories']\nfear_data = [] \nanger_data = []\njoy_data = []\nsadness_data = []\n\n# Storing and labelling data for each subreddit\nfor name in subreddit_names:\n    subreddit = reddit.subreddit(name)  \n    posts = subreddit.hot(limit=500)  \n    for post in posts:\n        post_words = post.selftext.split()\n        if len(post_words) in range(120, 600):\n            if name == 'scarystories':\n                fear_data.append({'text': post.selftext, 'label': 'fear'})\n            elif name == 'angry':\n                anger_data.append({'text': post.selftext, 'label': 'anger'})\n            elif name == 'funnystories':\n                joy_data.append({'text': post.selftext, 'label': 'joy'})\n            elif name == 'sadstories':\n                sadness_data.append({'text': post.selftext, 'label': 'sadness'})\n\n# Printing first 5 rows of data \nfor entry in fear_data[:5]:\n    print(entry)\n    print(\"-----------------------------------------------------------------------------------------------------------------------\")\nfor entry in anger_data[:5]:\n    print(entry)\n    print(\"-----------------------------------------------------------------------------------------------------------------------\")\nfor entry in joy_data[:5]:\n    print(entry)\n    print(\"-----------------------------------------------------------------------------------------------------------------------\")\nfor entry in sadness_data[:5]:\n    print(entry)\n    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n\n# Saving all the data in one list    \nall_data = fear_data + anger_data[:205] + joy_data[:205] + sadness_data[:205]","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': 'A few months ago, I moved into a new apartment. It was small, cheap, and perfect for my budget. The only weird thing? The apartment next to mine, 3B, always had this faint smell of something rotting. I figured it was just old food or maybe garbage that hadn’t been taken out, so I ignored it.\\n\\nOne night, around 2 a.m., I heard a faint tapping on my wall. I thought it was just the pipes, but then it started to sound like a soft, deliberate knocking. It was slow, rhythmic, and it didn’t stop. I knocked back, annoyed, and the tapping immediately stopped.\\n\\nThe next morning, I ran into my landlord in the hallway and casually asked about the neighbor in 3B. He looked at me with confusion and said, “Nobody’s lived in 3B for months. The last tenant disappeared without a trace, and no one’s moved in since.”\\n\\nThat night, I couldn’t sleep. Around 2 a.m., the tapping started again, but this time, it was louder and more urgent, like someone was pounding from inside the wall. I pressed my ear against the wall, heart racing, and I swear I heard someone whisper, “Help me.”\\n\\nTerrified, I called the cops. When they broke into 3B, it was completely empty, except for one thing—a small, crumpled note pinned to the wall with my name on it: “You’re next.”\\n\\nAs I stood there, frozen in fear, my phone buzzed. It was a text from an unknown number: “Why did you call them? Now they’re angry. They’re coming for you.”', 'label': 'fear'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'I’m not supposed to touch anyone. That’s been the rule as long as I can remember.\\n\\nMostly, my earliest memories are of my aunts and uncles, smiling at me behind glass in their white coats. Of air hugs and blown kisses.\\n\\nBut I have distant memories of another place. Of men with guns. Of a woman with a touch like silk. I think she was my mother. I remember a blinding light, and a sound like thunder. The next thing I knew, I was here. \\n\\nI’ve been behind glass for a long time. No windows to see outside. No door in or out, only a slot for meals. All my uncles and aunts have grown old and gray, yet I don’t seem to change.\\n\\nThey never answered my questions. \\n\\n“Who am I,” I would ask, “Where do I come from?” \\n\\nAll they ever told me is that I was sick. That I had to be kept locked away for my own safety, until a cure was found. Even though I felt fine, I trusted them. \\n\\nI had no choice. \\n\\nI’ve had all I ever needed here. I’ve read hundreds of books. Learned to paint and draw. Watched hours and hours of *Gilligan’s Island*. But what I always wanted more than anything was to touch someone, even for a second. \\n\\nI tried, once.\\n\\nA few years ago, one of my aunts was pushing supper through the meal slot. I backed against the wall like always, but I couldn’t help it. Without thinking, I ran forward and grabbed her hand. \\n\\nI wish I hadn’t. \\n\\nThe second my hand touched hers, her skin turned black and brittle, like burned paper. It crept up her arms until the flesh peeled off her skull, her eyes smoldering with yellow fire. Blood ran out of her mouth, *boiling*. She screamed and screamed, but all I could do was sob “I’m sorry” until security came. By the time they got to her, she was a shadow burned into the floor. \\n\\nThe meal slot became automated after that. \\n\\nI finally understood. I really was sick. *Very* sick. But it wasn’t *my* safety I was in here for. \\n\\nI thought I would be here forever, a freak in a cage. Until last week, when all my aunts and uncles gathered in the chamber one morning, with exciting news for me. \\n\\nI was cured. \\n\\nThey told me the country needed me. That some government men needed my help on a big mission, where I could touch as many people as I wanted. Men in rubber suits were going to take me away, to see the outside for the first time in forever. Just a few more days. \\n\\nI was so happy I couldn’t help but cry.\\n\\n“And before you go, we have a gift.”, said one of my uncles, a smile beaming across his face. \\n\\nI waited, excited and confused at once.\\n\\n“Your name is Mochitsura Yamamoto. You were born in a town called Nagasaki, in the year 1945…”  ', 'label': 'fear'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': \"A higher dimension is looking for a new portal for things to go into it and for things to come out of it. This higher dimension first wanted my eyes to be it's portal and at first it was all good. My eyes were a great portal but they became an annoyance. Like I could be using my eyes to watch something and then something comes out of my eyes from that higher dimension. At the same time when I looked at someone they could get pulled into the portal. It caused me great pain and because I was connected to the higher dimension, my negative feelings would affect it as well. \\n\\nThen the higher dimension chose my ears to be it's portal and at first it all worked out. I could my eyes and not worry about something coming in or out of that higher dimension. Then I remember talking with someone on the phone and then suddenly there was a third voice that could be heard by both of us on the phone. That was impossible as it was a private phone call between the two of us. The third voice told us where it came from. The third voice said that it had been reproduced by the sound of my voice and my friends voice. \\n\\nIt was one of the effects of my ears being the portal to a higher dimension. Anyone i listened to could get pulled into this higher dimension as well and things could also get out of my ears in the form of sound. This was no good and I wanted my ears back. So the higher dimension chose the space between my legs to be it's portal and I decided to go for it. Now anything that goes under my legs would get dragged into that portal and things could also come out of the space between my legs from that other higher dimension. \\n\\nIt was still chaos as objects got dragged into that higher dimension and random things also came put. So I decided to cut half my body through operation. So now my legs are separate from my body and it is an easier life. When I woke up from operation, it was just my body arms and head. Some of the surgeons and nurses got dragged between the space of my legs. \\n\\nI feel like my life is much better now and I hope it doesn't get worse. The higher dimension got to have its portal and I get to live a normal life. \", 'label': 'fear'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'Hi peoples \\n\\nI’ve been dealing with some creepy noises at night ever since my new neighbors moved in, and it’s starting to really get to me. Every few nights usually around 2am but it has happened at various hours of the night and only ever at night. I work from home quite a bit and have never heard it in the day. I hear what sounds like a man in heavy boots running along the fence between our houses. The fence is right outside my window, so the noise is incredibly loud and hard to ignore.\\n\\nThe sound is too deliberate and heavy to be an animal; it genuinely feels like someone is sprinting with purpose. It generally some with unnerving rustling, and a couple of times, I’ve heard a loud thud, like whoever or whatever it is has actually hit the fence or what I’m anxious about, is it could be them jumping the fence.\\n\\nThere’s a family next door and while there is a man who lives there, I can’t shake this uneasy feeling that something isn’t right. I’m half-tempted to peek over the fence to see what’s going on but the thought of what I might or worse, being caught looking makes me hesitate.\\n\\nThis whole situation is really unsettling and I find myself lying awake for hours after waking up to it. Has anyone else experienced something like this? What would you do in my shoes? I’m losing sleep and could really use some advice on how to handle this. Or even advice on what you think it could be. ', 'label': 'fear'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': '\\nI recently got the AI Interstellar Dreamscape iPhone case, and from the moment I snapped it on, I couldn’t stop admiring it. Made in Germany, this case features a stunning design of vibrant galaxies, glowing nebulae, and distant stars—it’s like holding a piece of the cosmos in your hand. The case isn’t just beautiful; it’s durable too, offering strong protection that makes you feel confident your phone is safe, no matter where the day takes you.\\n\\nAfter a few days, I realized there was something almost magical about it. I found myself getting lost in the cosmic patterns, feeling transported into the vastness of space whenever I looked at my phone. Friends would notice it instantly, drawn to its vivid colors and intricate design. And at night, I’d dream of drifting through space, surrounded by the same breathtaking galaxies and stars, feeling a unique blend of peace and excitement.\\n\\nWhat makes it even better is the peace of mind that comes with it. With a 30-day no questions asked return policy, I knew I had nothing to lose. But honestly, once you see this case in person, you won’t want to part with it. If you’re looking for something that combines beauty, quality, and a bit of cosmic magic, you need the AI Interstellar Dreamscape case.\\n\\nOrder yours today and experience the universe in your pocket, risk-free.\\n\\nhttps://stayingaheadofai.com/products/the-ai-interstellar-dreamscape-iphone-case', 'label': 'fear'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'hello everyone I (19F)  go to a cosmetology school it wasn\\'t my number 1 career path but I had to do something because culinary was Hella expensive \\nfor some background I started this Last year after being done with high school in 2022 we found this school which offered beauty lessons it was established in 2010 and when we researched it,it said it was an accredited school but now it doesn\\'t \\n\\nthe school wasn\\'t really the best as most of the time there would only be 5 out 30 students and there would only be one teacher that does all the modules (eg nails make up facials eyelashes massages waxing) she only had a qualification in nails and even then I had some doubts me and some other ladies have tried to speak to the owner about this but she ignores us \\n\\nI stopped going and paying but I think the damage had already been done I mean I\\'ve spent thousands of money in this. \\n\\na month ago my school appeared in the newspaper tittled as \"Fly by night school\" we had the owner come and explain herself but she told us she\\'s moving from the organization Seta to qtco that was that. \\n\\non Sunday the school appeared on TV by a show that exposes the biggest crooks and scams in the country and we didn\\'t get any updates\\nthe show said that the school wasn\\'t registered or accredited with anything\\nwe going to meet her tomorrow ', 'label': 'anger'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'I work as a stock handler on third shift at a factory. Sunday night through Thursday night. Went to work like usual Sunday night. Shut my mouth and did my job. Didn’t really talk to anybody at all and nobody really talked to me either which i don’t mind because i enjoy the peace and quiet. Got off work and came home and went to bed because i was exhausted. Woke up this afternoon getting a phone call from the HR lady at the company leaving me a voicemail telling me that they are suspending me without pay while they are conducting an investigation so i answered. I asked what this was about and all she told me is that she couldn’t tell me anything because there is an ongoing investigation. I told her that i’ve done nothing at all. I’m lost. I have no idea as to what i did to anyone or anything at all. I go to work, shut my mouth, do my job and that’s it. I have had attitude problems in the past and i have been wrote up for it once before but after that i decided it just wasn’t worth it anymore so i stopped and i’ve just focused on doing my job for the day and that’s it. I’m there to do my job get paid for it and go home. I’m not there to make friends or be a “family” like the company wants to try and brainwash people into thinking that we are. We are coworkers and that’s it. Nobody has approached me to try and talk to me so this suspension has blindsided me because i just have no idea as to what i did to deserve this kind of treatment. I’ve worked there for over 8 years and i feel as though i do an exceptional job everyday that im there. I’m furious that im stuck losing out on getting paid for however long they decide to suspend me while they do this investigation when i have no idea at all as to what ive done and they won’t tell me at all either. Its complete bullshit', 'label': 'anger'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': \"Ok so I love my friends dearly but they are so unbelievably disrespectful to my mom and new step dad idfk why like WHY because my mom is very chill and aslong as we are safe and not doing damage she doesn't care what we do and doesn't hesitate to do things for us and thare behavior is making my chest hurt I currently stuck with my friend in my house and they keep rolling thare eyes and my mom for making the smallest amount of noise and the next time they do it ima back hand them just incase you don't think eye rolling isn't enough for my anger I'll give a list (back talk,standing over my mom trying to intimidate,disobeying small tasks like pick your drink up it's spilling,being ruff with priceless or expensive stuff,sneaking drinks under her supervision when we are under age,and more😡) so idk really close to beating my friends and dealing without any\\nAlso sorry for lack of grammar and spelling \", 'label': 'anger'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'Holy ship this one trucking kid knows like everyone and is friends with everyone but guess WHAT my friend makes a fat joke to our other friend who doesn’t mind (he’s a slob on risk of diabetes and barely sleeps)\\nAnd this truckin guy has the AUDACITY TO LOOK ME UP AND DOWN 5 TIMES (reminder I’m a but chubby that’s it but I still could lose a bit) and other times he’s like Shut up OP for no reason and when he says the most random ship it’s okay but when j barely say something that’s barely out of pocket he’s like that ones not funny or wth or even stop talking funning brainrot if I say something like alpha but he says sigma Ohio grandrizzker man of the thousand sigmas ITS OKAY AND EVERYONE LAUGHS and something he found off TikTok..\\nWORST OF QLL IS HE SAID “oh I forgot you were here last year” LIKE WHAT THE F##K BRO HATEZ ME OR SOMETHING\\nAnd 2 of my main 4 friends are friends with him\\nwhat should I do?', 'label': 'anger'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'I’m quite short tempered these days (for obvious reasons) and most of the time, I’m raging on the inside.\\n\\nSo, here goes..\\n.\\n.\\n. \\n. \\n\\n\\nFuck you, M***** and N*****. I don’t even wanna call you mum and dad. You guys are ruining my young adult years just as you have destroyed my teen years and childhood. \\n\\nI hope you guys die soon in the worst possible ways/situations/places and rot in the deepest layer of hell. I hate you guys so much. You invading my personal space or getting intrusive and telling strangers about my personal stuff, these are never gonna work. Your religious beliefs can’t affect me either. Even God is angry at you. Look what I got and you got none of those things. I hope you motherfuckers are cursed for life. \\n\\nTo M*****: Your jealousy surrounding me and my ability to earn and being independent is pointless. I’ll be better than you. Also, don’t try to call me ugly when you look like that, motherfucker. You codependent, narcissistic pos. Go suck your wife’s tits. You despicable human being. \\n\\nTo N*****: Your bitterness and jealousy surrounding my beauty and youth is a massive fucking bs. You’re the one who didn’t let me wear those nice things but I’ll wear them, for sure (I did that and I will continue to wear them in the future). You ugly piece shit are trying to age shame me? I’m young. The one who’s old is you. Go fuck yourself cause no one will (other than that ugly mf). \\n\\nI thought I’d be raging while writing this but I’m unbelievably calm. \\n\\nAlright. That’s it 🤷🏻\\u200d♀️ ', 'label': 'anger'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'That’s one way to scare off religious solicitors. \\n\\nSo, back in 2011, I was majoring in theatre arts at the local college and was in the theatrical makeup class. It was the only class I had on Fridays and I frequently came home in whatever I had done that day in class to show my dad. \\n\\nIt was the week of Halloween and our teacher was teaching us “bloodies” (gore makeup), and my horror-obsessed, haunted house scare actor, “Fleet Street is the best stage blood” boasting self was going WILD. While most of the other students did a burn, a few bruises, maybe a gash on the arm…I made myself look like roadkill that also got mauled by a bear. It was A LOT, enough to get me pulled over by a concerned campus cop when I was leaving. I went home covered in fake blood, looking like hell and LOVING IT.\\n\\nMy dad told me to keep it on so my niece and nephews could see when they got there after school (they knew I had makeup class, I wasn’t going to scar them). So around 2:45pm, I hear a knock at the door. Well I decided to answer like a zombie to make them laugh. \\n\\nI should note: I realize I should have probably checked to make sure it was them…\\n\\nI answer the door slowly, dragging a foot, moaning “brains”…to see two Jehovah’s Witnesses go green and absolutely BOLT back to their bikes. I literally was still processing what had just happened as they peeled around the corner and my brother showed up with the kids…\\n\\nTo this day, my dad says he sees them actively avoid our door and once heard them refer to it as “the home of the devil.” ', 'label': 'joy'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'Another story about people from my hometown. \\n\\nLast story: Kim the mechanic. \\n\\nSo in my hometown, there was a woman known as “Black Betty” she was known on the streets as the “plus sized woman in overalls who stole from the old sugar daddy men.” \\n\\nAnyway, Black Betty was looking to make some extra money, and in my hometown. There’s some places that people have let go, so they’re run down and abandoned. \\n\\nWell, Black Betty went into one of these parking lots, and some of the parking meters would wobble around and the hole in the ground would be larger than the pole. \\n\\nShe was at this parking meter and she pulled the whole meter up out of the ground, took off and took it to the junk yard. \\n\\nShe gets to the junk yard and asks them to help her get the money out of it. They tell her they will in a minute if she could wait outside. \\n\\nThey call the police, and they show up to the junk yard. There’s meter Betty standing by the door with the parking meter in her hands. \\n\\nThe city went and took up all the loose parking meters after that, she got released from jail and went back to robbing old men who picked her up in the street. ', 'label': 'joy'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'So, back in 2011, I was majoring in theatre arts at the local college and was in the theatrical makeup class. It was the only class I had on Fridays and I frequently came home in whatever I had done that day in class to show my dad. \\n\\nIt was the week of Halloween and our teacher was teaching us “bloodies” (gore makeup), and my horror-obsessed, haunted house scare actor, “Fleet Street is the best stage blood” boasting self was going WILD. While most of the other students did a burn, a few bruises, maybe a gash on the arm…I made myself look like roadkill that also got mauled by a bear. It was A LOT, enough to get me pulled over by a concerned campus cop when I was leaving. I went home covered in fake blood, looking like hell and LOVING IT.\\n\\nMy dad told me to keep it on so my niece and nephews could see when they got there after school (they knew I had makeup class, I wasn’t going to scar them). So around 2:45pm, I hear a knock at the door. Well I decided to answer like a zombie to make them laugh. \\n\\nI should note: I realize I should have probably checked to make sure it was them…\\n\\nI answer the door slowly, dragging a foot, moaning “brains”…to see two Jehovah’s Witnesses go green and absolutely BOLT back to their bikes. I literally was still processing what had just happened as they peeled around the corner and my brother showed up with the kids…\\n\\nTo this day, my dad says he sees them actively avoid our door and once heard them refer to it as “the home of the devil.” ', 'label': 'joy'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': \"So, I work at a restaurant, and the other night we had this sweet elderly couple come in for dinner. They finished their meal, and the gentleman excused himself to the restroom. No big deal, right? Well, he was in there for like, and hour until his wife finally went in to retrieve him. Fast forward a few hours—now we're closing up, and it’s my turn to clean the bathroom.\\n\\nI walk in, and oh my gosh, the horror. The toilet was clogged to the brim with poop, the trash can was overflowing with paper towels smeared with—you guessed it—poop, and there was poop on the floor. And the smell. Oh, the smell. I’m telling you, I couldn't even. \\n\\nSo, my coworker Dan (bless his soul) took one for the team. He unclogged the toilet, wiped up the poop on the floor, and I had to tackle the trash can. I pulled out the bag, and lo and behold, there was a rogue turd hiding underneath the trash bag in the trash can. I don't even want to know how that happened.\\n\\nEven after Dan wiped down the walls with bleach, the bathroom reeked for days. It’s only just starting to smell normal, but it was so bad you could smell it in the dining room. One of the cooks has dubbed this guy “The Phantom Shitter.” So yeah, that was our night. Hope someone finds this as hilarious as we did (after the trauma subsided, of course).\", 'label': 'joy'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'So, here’s a strange story from a couple of years ago that I’ve never really shared with anyone because, well, it’s kind of embarrassing but also hilariously absurd. I was living in Jersey at the time, and one Saturday afternoon, I decided to take a walk in the woods near the highway.\\n\\nWhy was I in the woods, you ask? I wasn’t exactly the outdoorsy type, but I had this phase where I just needed to get away from screens, my phone, and the noise of life. You know, just be one with nature. Plus, there was a local trail that had a reputation for being pretty peaceful, and I figured a walk could clear my head. \\n\\nSo, I’m walking along, minding my own business, when I notice something half-buried in the mud. At first, I thought it was just trash—Jersey is notorious for litter along highways, so it wouldn’t have been surprising. But as I got closer, I realized it was a DVD case. \\n\\nIt was so filthy that at first, I didn’t even realize what it was. I honestly thought the cover was black. Curiosity got the best of me, so I picked it up. After wiping off some of the dirt, I realized it wasn’t just any DVD—it was some adult movie. The title was barely legible, but the characters on the cover had distinctly Indian features. I couldn’t help but laugh at how random this was. \\n\\nHere I was, trying to disconnect from modern life, only to find some kind of Bollywood-themed adult flick, of all things, half-buried in the mud. The case was still intact, which made me think that whoever dropped this either lost it during a hike (which is already a weird thought) or intentionally left it there, maybe hoping someone else would stumble upon it. It’s almost like it was meant to be found.\\n\\nThe rest of my walk was filled with ridiculous thoughts about who might have brought this out here. Maybe it was someone who had a weird idea of what a nature walk should involve? Maybe it was part of some bizarre scavenger hunt? \\n\\nWhen I got home, I couldn’t resist. I popped the DVD into my old player, eager to see what kind of hilarity I’d uncovered. But here’s the kicker—the damn thing didn’t work. Not even a flicker. It must have gotten wet at some point because the disc looked fine at first, but once it started spinning, the player just spat it out. \\n\\nPart of me was disappointed. I mean, after all that buildup, it could have been the most bizarrely entertaining thing I’d ever seen. But another part of me was relieved because maybe it’s better not to know what kind of weirdness that DVD contained. \\n\\nAnd so, it sits on my shelf now, a bizarre souvenir from that random day in the woods. Sometimes I think about tossing it, but then I remember how strangely funny and absurd that moment was, and I just can’t bring myself to get rid of it.\\n\\nWhoever dropped that thing in the woods, whether by accident or on purpose, definitely gave me a story to tell.', 'label': 'joy'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': \"I had a friendship with a girl for 5-6 years, we talked for a few months, after about 2 years we didn't talk again, and in the last 3 we talked even if we had a few weeks gap, but at least we knew each other and unfortunately we didn't too much attention, 3 years ago we met for the first time and she is the most precious person I know, she is very lovely, very hardworking, very beautiful and super smart, we have similar musical tastes, and we had a crush for a long time since we f2f, but I couldn't take a step because, distance, distance is killing me, I've had long distance relationships before, and they all ended badly, and I didn't want this to happen and with her, I was also friends with her, and I loved her so much that I wouldn't have risked losing her in any way, even if that meant not telling her what I felt, because I was afraid of being rejected and don't talk to me anymore, recently he kept telling me about a boy, and he talked about him as if he was the perfect boy, and he kept talking about him, and it just hurt me a lot to see how he talks about that boy, he's going to see with him, and unfortunately 3 days ago I had a small argument with her and she doesn't answer me anymore, before that I talked to her friend that I like her, and now she doesn't answer me anymore, and now I regret it, I regret not I told him how I felt because I was afraid he would reject me and we wouldn't talk anymore, but afterwards, I didn't tell him what I felt and maybe he found out and doesn't know what he wants (maybe) and now not anymore we're talking, a few hours have passed and that's all I'm thinking about, I still want an opinion or an advice, I really can't afford to lose this girl, I don't know what I'd do, it would be the last straw, and the glass is almost spilled..\", 'label': 'sadness'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': '(This is a fictional story it’s not supposed to be real I’m just using ‘my, I and me’ wording)\\n\\nMy mom was hit hard with grief after my dad died. I was a baby and she tried everything she could to make sure I knew I had a dad who loved me even though he was gone. We visited every 3rd Tuesday of every month. She told me stories about him, told me how he smiled and all of the above. As I got older I realized how hard this hit my mom. She showed me a teddy bear he got me for my ‘welcome to earth’ gift. Even though he was sick in bed with an illness they couldn’t cure. As she got older she got weaker and couldn’t make the trip to his grave eventually. When she died we put her grave right next to his. And when me and my sister died… we all got to live together in heaven. Our own house.: was like we were all normal again.', 'label': 'sadness'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'heres a story i would love you to see. Don\\'t forget through out this story im dyslexic and have autism, it starts with me getting a girlfriend named amelia,now she was great but one day,she got cancer and she was fighting for her life but she sadly passed away,now this hit me like a brick . i started to smoke,drink do drugs and then i met someone named luna,she was great until i told her that she reminds me of my ex named amelia,clearly not happy she started to emotionally abuse me, to the point where i almost committed suicide at the end of this \"relationship\"and she made me develop depression .so yet again i meet another girl named angie,she was perfect and always there,she knew what i tried to do, everything was going great,until one day her parents found out us and they were furious,they made her block me and convince them im a bad man,sadly she became delusional and told me to try kill myself totally aware of what happened months prior,and AGAIN i met a new girl named alexis,and we are still together and she has tourettes, thats the end of the story and i would love if you send support', 'label': 'sadness'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'Trigger warning, sad story, potential child endangerment.\\n\\nThis is an infamous [story](https://allthatsinteresting.com/4-children-for-sale) in which a family [sold their children](https://www.youtube.com/shorts/7Ytb3bQEc60).\\n\\nIt was 1948. The post WW II era was a good economic era in the USA. We don\\'t know the [full story](https://nypost.com/2013/07/14/finding-peace-in-a-life-sold-for-2/). Perhaps the husband could not keep a job. (Edit: I put some links which told more. It\\'s worse than I had imagined.)\\n\\nToday the children might not be \\'sold\\' but in some parts of the world I have heard about this happening if a parent could not feed the children. In other countries the children might simply be \\'left with\\' a relative or a friend, to uncertain fates.\\n\\nOther parents would drop their children off at an orphanage, if they couldn\\'t feed them. Social programs and other things were not what they are today. I\\'ve seen stories from the 19th century in which one parent dies or leaves and the other drops their kids at an orphanage. Some orphanages also accepted children during daytime hours, if the single parent worked outside home.\\n\\nThe mother in the video was pregnant again. Did she not have access to BC, or her husband forbid it, or it was too expensive, or? Was she depressed? She was only 24 in this image but looked middle-aged. Her husband escaped scrutiny and was not in the photo.\\n\\nI want to reach back through time and drop off groceries or something. This is brutally sad.\\n\\nTL/DR the father abandoned them; he was 16 years older than the mother; but the mother prioritized herself and her \\'new man\\' over the children. She later remarried, and kept her four resulting \\'new\\' children.\\n\\nThe four children who were sold (one for $2 for \"Bingo money\"), reunited in later life, as adults. So they got to meet again, at least. More details about the 5 \\'sold\\' children at the links.', 'label': 'sadness'}\n","-----------------------------------------------------------------------------------------------------------------------\n","{'text': 'It was a winter night.\\n\\nA small nymph of a girl made shelter behind a nest of bins. It was hardly enough though. Very…oh so very cold. Threadbare hung on her gaunt figure, her hair slicked back with sweat, soot and now-\\n\\nShe looked up at the sky.\\n\\nSnow.\\xa0\\n\\nThe harsh air bit at her skin. She clutched herself tighter.\\n\\nA mum, or dad…She stared at the surrounding houses’ windows, lit by candle light. Warmth.\\n\\nShe lowered her eyes in an effort to not deceive herself.\\xa0\\xa0\\n\\nNo matter what she scrounged together - be it bins or street litter - her makeshift clothes were not enough. It would never be, against the natural elements. Her pale face grew red from the harsh stings of the winds.\\n\\nAny tears felt like dried icicles. Her throat rubbed raw to speak much.\\n\\nBut then a bell rang. She held her breath, as dull footsteps made their way down the narrow street path.\\n\\n*Was it a caretaker? A warden?*\\n\\nHer feeble bones started to shake in fear. She couldn’t run.\\n\\nShe couldn’t-\\n\\nPeering ever more closely, she took in the figure.\\n\\nA man.\\n\\n‘Though not really so,’ she decided. He looked too slim, not too tall; his face betraying his youth. Trudging closer, he held out an apple.\\xa0\\n\\nLike a snake, she pounced to take it. Sudden energy flooding her at the promise of food. Her eyes, locked in at the apple, made her nearly miss the other object he held out to her.\\n\\nA blanket.\\n\\nShe reached out once more, before halting abruptly.\\xa0\\n\\nThe boy didn’t seem to have much either.\\n\\nIn a crackled whisper of a voice, she questioned, “And you?”\\n\\nHe shook his head slowly, giving the briefest of smiles.\\n\\nSeemingly satisfied, he turned, walking away. Not once looking back.\\n\\nFor if he did he’d have noticed the faint glimmer of hope that now sketched into her eyes. Her stance that now sat stronger, more composed.\\xa0\\n\\n*More willing to survive.*\\n\\nBut that was okay.\\n\\nOne doing so was enough for the both of them.\\xa0', 'label': 'sadness'}\n","-----------------------------------------------------------------------------------------------------------------------\n"]}],"source":["# Initialize the Reddit client\n","reddit = praw.Reddit(\n","    client_id='u6EMlVDn7jAwSKDA1A17Ww',\n","    client_secret='X2HbDELH7VjM6yOkRv9HXMJf1VcQXQ',\n","    user_agent='sentiment_app'\n",")\n","\n","# Creating a list for the subreddit names and for storing the data\n","subreddit_names = ['scarystories', 'angry', 'funnystories', 'sadstories']\n","fear_data = [] \n","anger_data = []\n","joy_data = []\n","sadness_data = []\n","\n","# Storing and labelling data for each subreddit\n","for name in subreddit_names:\n","    subreddit = reddit.subreddit(name)  \n","    posts = subreddit.hot(limit=500)  \n","    for post in posts:\n","        post_words = post.selftext.split()\n","        if len(post_words) in range(120, 600):\n","            if name == 'scarystories':\n","                fear_data.append({'text': post.selftext, 'label': 'fear'})\n","            elif name == 'angry':\n","                anger_data.append({'text': post.selftext, 'label': 'anger'})\n","            elif name == 'funnystories':\n","                joy_data.append({'text': post.selftext, 'label': 'joy'})\n","            elif name == 'sadstories':\n","                sadness_data.append({'text': post.selftext, 'label': 'sadness'})\n","\n","# Printing first 5 rows of data \n","for entry in fear_data[:5]:\n","    print(entry)\n","    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n","for entry in anger_data[:5]:\n","    print(entry)\n","    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n","for entry in joy_data[:5]:\n","    print(entry)\n","    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n","for entry in sadness_data[:5]:\n","    print(entry)\n","    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n","\n","# Saving all the data in one list    \n","all_data = fear_data + anger_data + joy_data + sadness_data"]},{"cell_type":"code","execution_count":3,"id":"cd5f02cf-984a-4b0a-b1c7-5400a3cdecba","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1722944779782,"lastExecutedByKernel":"9dc7d9fd-0e7c-4d21-a43a-72c69d455cfa","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(len(fear_data))\nprint(len(sadness_data))\nprint(len(joy_data))\nprint(len(anger_data))","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["167\n","266\n","250\n","204\n","887\n"]}],"source":["# Printing the total number of data extracted from each subreddit\n","print(len(fear_data))\n","print(len(sadness_data))\n","print(len(joy_data))\n","print(len(anger_data))\n","print(len(all_data))"]},{"cell_type":"code","execution_count":4,"id":"832857bf","metadata":{},"outputs":[],"source":["label_map = {\"joy\": 0, \"anger\": 1, \"fear\": 2, \"sadness\": 3}\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()  \n","\n","# Function to preprocess the sentences\n","def preprocess_sentences(data):\n","    processed_sentences = []\n","    processed_data = []\n","    for item in data:\n","        sentence = item['text'].lower() # Convert all characters into lower case\n","        # Replace any new line characters with a space\n","        sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('\\xa0\\n\\n', ' ') \n","        # Remove any special charaters other than the ones listed\n","        sentence = re.sub(r'[^\\w\\s,:\\'\\\"()?.,!;-{}\\*_]', '', sentence)\n","        tokens = word_tokenize(sentence) # Tokenizes the sentence\n","        tokens = [token for token in tokens if token not in stop_words] # Removes stop words\n","        tokens = [stemmer.stem(token) for token in tokens] # Stems all the words\n","        processed_sentence = ' '.join(tokens) # Joins tokens to form the final sentence\n","        processed_sentences.append(processed_sentence) # Adds the final sentence to the list\n","        label = label_map[item['label']]\n","        processed_data.append({\"text\": processed_sentence, \"label\": label})\n","    return processed_data\n","\n","# Function to encode the sentences using Tf-idfVectorizer\n","def encode_sentences(data):\n","    sentences = [item['text'] for item in data]\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(sentences)\n","    labels = [item['label'] for item in data]\n","    return X.toarray().astype(np.float32), labels, vectorizer\n","\n","# Class to create our custom dataset\n","class EmotionDataset(Dataset):\n","    def __init__(self, x_data, y_data):\n","        self.x_data = torch.tensor(x_data, dtype=torch.float32)  # Convert features to tensors\n","        self.y_data = torch.tensor(y_data, dtype=torch.long)  # Convert labels to tensors\n","    def __len__(self):\n","        return len(self.x_data) # Returns the length of the data\n","    def __getitem__(self, idx):\n","        text = self.x_data[idx]\n","        label = self.y_data[idx]\n","        return {\"text\": text, \"label\": label} # Returns the text and it's label\n","\n","# Function to implement the text processing pipeline\n","def text_processing_pipeline(data, test_size=0.2, batch_size=16):\n","    processed_data = preprocess_sentences(data) # Preprocesses the texts\n","    encoded_sentences, labels, vectorizer = encode_sentences(processed_data) # Encodes it\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(encoded_sentences, labels, test_size=test_size, random_state=42)\n","    # Create datasets\n","    train_dataset = EmotionDataset(X_train, y_train)\n","    test_dataset = EmotionDataset(X_test, y_test)\n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    return train_loader, test_loader, vectorizer\n","\n","train_loader, test_loader, vectorizer = text_processing_pipeline(all_data)"]},{"cell_type":"code","execution_count":5,"id":"1b1a35bc","metadata":{},"outputs":[],"source":["# My own model\n","class EmotionRecognitionModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_prob=0.5):\n","        super(EmotionRecognitionModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout_prob = dropout_prob\n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        # Bidirectional LSTM\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        # Dropout layer\n","        self.dropout = nn.Dropout(p=self.dropout_prob)\n","        # Layer normalization\n","        self.layer_norm = nn.LayerNorm(hidden_size * 2)  # Hidden size * 2 for bidirectional\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Output size is hidden_size * 2 for bidirectional\n","\n","    def forward(self, x):\n","        # x contains word indices, we first convert them to embeddings\n","        x = self.embedding(x)\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n","        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n","        # Forward pass through LSTM\n","        out, _ = self.lstm(x, (h0, c0))\n","        # Take the output from the last time step\n","        out = out[:, -1, :]\n","        # Apply dropout\n","        out = self.dropout(out)\n","        # Apply layer normalization\n","        out = self.layer_norm(out)\n","        # Fully connected layer\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"code","execution_count":17,"id":"cb09be66-cbb8-4d5d-b5fa-a514275f8dda","metadata":{"executionCancelledAt":null,"executionTime":860,"lastExecutedAt":1722944785867,"lastExecutedByKernel":"9dc7d9fd-0e7c-4d21-a43a-72c69d455cfa","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initializing important variables that will be used for training and testing the model\n\nvocab_size = len(vectorizer.get_feature_names_out())\nembedding_dim = 100\nhidden_size = 128\nnum_layers = 3\nnum_classes = 4\n\nmodel = EmotionRecognitionModel(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\naccuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\nf1 = F1Score(task=\"multiclass\", num_classes=num_classes)"},"outputs":[],"source":["# Initializing important variables that will be used for training and testing the model\n","\n","vocab_size = len(vectorizer.get_feature_names_out())\n","embedding_dim = 300\n","hidden_size = 256\n","num_layers = 2\n","num_classes = 4\n","\n","model = EmotionRecognitionModel(vocab_size, embedding_dim, hidden_size, num_layers, num_classes) # Initializing the model\n","model.to('cuda') # Sending it to the GPU\n","criterion = nn.CrossEntropyLoss() # Initializing the loss function\n","optimizer = AdamW(model.parameters(), lr=1e-5) # Initializing the optimizer\n","\n","accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes).to('cuda') # Initializing the accuracy metric\n","f1 = F1Score(task=\"multiclass\", num_classes=num_classes).to('cuda') # Initializing the F1-score metric"]},{"cell_type":"code","execution_count":18,"id":"3fe23e5c-9d3c-4e36-b4ba-e737b7eac3dd","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/10: 100%|██████████| 45/45 [01:59<00:00,  2.66s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 1.4407\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/10: 100%|██████████| 45/45 [01:59<00:00,  2.65s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 1.4526\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/10: 100%|██████████| 45/45 [02:00<00:00,  2.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 1.4299\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/10: 100%|██████████| 45/45 [01:58<00:00,  2.64s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 1.4368\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/10: 100%|██████████| 45/45 [02:02<00:00,  2.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 1.4281\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/10: 100%|██████████| 45/45 [02:00<00:00,  2.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 1.4297\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/10: 100%|██████████| 45/45 [02:00<00:00,  2.67s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 1.4363\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/10: 100%|██████████| 45/45 [01:59<00:00,  2.67s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 1.4392\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/10: 100%|██████████| 45/45 [01:59<00:00,  2.67s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 1.4274\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/10: 100%|██████████| 45/45 [01:59<00:00,  2.67s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 1.4265\n","Finished Training\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]"]},{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.2697\n","Test F1 Score: 0.2697\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Training function\n","def train_model(model, train_loader, criterion, optimizer):\n","    model.train() # Putting the model in training mode\n","    for epoch in range(10):\n","        running_loss = 0.0\n","        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/10\")\n","        for i, batch in progress_bar:\n","            texts = batch['text'].long().to('cuda')\n","            labels = batch['label'].long().to('cuda')\n","            optimizer.zero_grad()  # Resetting gradients          \n","            outputs = model(texts) # Forward pass to get the output probabilities\n","            loss = criterion(outputs, labels)  # Calculating loss\n","            loss.backward()  # Back propagation\n","            optimizer.step()  # Performing an optimizer step\n","            running_loss += loss.item() * texts.size(0)  # Calculating the running loss      \n","        epoch_loss = running_loss / len(train_loader.dataset)  # Calculating the total loss for each epoch\n","        print(f'Epoch {(epoch+1)}/10, Loss: {epoch_loss:.4f}')\n","    print('Finished Training')\n","\n","# Evaluation function\n","def evaluate_model(model, test_loader):\n","    model.eval() # Putting the model in evaluation mode\n","    all_labels = []\n","    all_predictions = []\n","    progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            texts = batch['text'].long().to('cuda')\n","            labels = batch['label'].long().to('cuda')       \n","            outputs = model(texts)  # Forward pass to get the output probabilities\n","            _, predicted = torch.max(outputs, 1)  # Getting the class with maximum probability           \n","            # Moving all the labels and predictions to the CPU \n","            # as NumPy cannot directly access them if they're in the gpu\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predicted.cpu().numpy())\n","            # Calculating the accuracy and f1 score\n","            accuracy.update(predicted, labels)\n","            f1.update(predicted, labels)\n","    accuracy_value = accuracy.compute()\n","    f1_value = f1.compute()\n","    print(f'Test Accuracy: {accuracy_value:.4f}')\n","    print(f'Test F1 Score: {f1_value:.4f}')\n","\n","    accuracy.reset()\n","    f1.reset()\n","    \n","# Run training and evaluation\n","train_model(model, train_loader, criterion, optimizer)\n","evaluate_model(model, test_loader)"]},{"cell_type":"code","execution_count":9,"id":"02e200e9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Data Preprocessing for the DistilBERT Model\n","\n","label_map = {\"joy\": 0, \"anger\": 1, \"fear\": 2, \"sadness\": 3}\n","\n","# Preprocessing function (to only lowercasing and special character removal)\n","def preprocess_sentences(data):\n","    processed_data = []\n","    for item in data:\n","        sentence = item['text'].lower()\n","        sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('\\xa0\\n\\n', ' ')\n","        sentence = re.sub(r'[^\\w\\s,:\\'\\\"()?.,!;-{}\\*_]', '', sentence)\n","        processed_data.append({\"text\": sentence, \"label\": label_map[item['label']]})\n","    return processed_data\n","\n","# Dataset class for DistilBERT\n","class EmotionDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __len__(self):\n","        return len(self.data) \n","    \n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        label = self.data[idx]['label']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Text processing pipeline \n","def text_processing_pipeline(data, test_size=0.2, batch_size=16):\n","    processed_data = preprocess_sentences(data) # Preprocesses sentences\n","    # Splitting the data into training and testing sets\n","    train_data, test_data = train_test_split(processed_data, test_size=test_size, random_state=42)\n","    # Instantiating the DistilBERT tokenizer\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    # Instantiating the training and testing data for the DistilBERT model\n","    train_dataset = EmotionDataset(train_data, tokenizer)\n","    test_dataset = EmotionDataset(test_data, tokenizer)\n","    # Instantiating the dataloader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    \n","    return train_loader, test_loader\n","\n","# Instantiating the training and testing data for the DistilBERT model\n","train_loader, test_loader = text_processing_pipeline(all_data)"]},{"cell_type":"code","execution_count":12,"id":"f624e177","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 46/46 [00:06<00:00,  7.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 1.3461448653884556\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 1.1806268018224966\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.9593151017375614\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.7333062031994695\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.5332187770501428\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.37418886630431464\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.2720900989421036\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.16475849566252335\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.10784011706709862\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:05<00:00,  7.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.08221789514241011\n","Accuracy: 0.7514\n","F1-Score: 0.7499\n"]}],"source":["# Instantiating the model\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n","model.to('cuda') # Moving the model to the GPU\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5) # Instantiating the optimizer\n","\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=4).to('cuda') # Instantiating the accuracy metric\n","f1_metric = F1Score(task=\"multiclass\", num_classes=4, average='weighted').to('cuda') # Instantiating the f1 metric\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train() # Setting the model to training mode\n","    total_loss = 0\n","    for batch in tqdm(train_loader):\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        labels = batch['label'].to('cuda')\n","        optimizer.zero_grad() # Resetting gradients\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # Forward pass to get the output probabilities\n","        loss = outputs.loss # Calculating loss\n","        total_loss += loss.item() # Calculating the running loss\n","        loss.backward() # Back propagation\n","        optimizer.step() # Performing an optimizer step\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'distilbert_emotion_detection.pth')\n","\n","model.eval() # Seting the model to evaluation mode\n","accuracy_metric.reset() # Resetting the accuracy metric\n","f1_metric.reset() # Resetting the f1 metric\n","\n","# Evaluate the model\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        labels = batch['label'].to('cuda')\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Forward pass to get the output probabilities\n","        preds = torch.argmax(outputs.logits, dim=1) # Getting the class with the highest probability\n","        # Updating the metrics\n","        accuracy_metric.update(preds, labels)\n","        f1_metric.update(preds, labels)\n","\n","# Compute final scores\n","accuracy = accuracy_metric.compute().item()\n","f1 = f1_metric.compute().item()\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")"]},{"cell_type":"code","execution_count":13,"id":"d5427645","metadata":{},"outputs":[],"source":["# Text preprocessing for the RoBERTa model\n","label_map = {\"joy\": 0, \"anger\": 1, \"fear\": 2, \"sadness\": 3}\n","\n","# Preprocessing function (lowercasing and special character removal)\n","def preprocess_sentences(data):\n","    processed_data = []\n","    for item in data:\n","        sentence = item['text'].lower()\n","        sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('\\xa0\\n\\n', ' ')\n","        sentence = re.sub(r'[^\\w\\s,:\\'\\\"()?.,!;-{}\\*_]', '', sentence)\n","        processed_data.append({\"text\": sentence, \"label\": label_map[item['label']]})\n","    return processed_data\n","\n","# Dataset class for RoBERTa\n","class EmotionDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        label = self.data[idx]['label']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Text processing pipeline \n","def text_processing_pipeline(data, test_size=0.2, batch_size=16):\n","    processed_data = preprocess_sentences(data) # Preprocessing the data\n","    # Splitting the data into training and testing sets\n","    train_data, test_data = train_test_split(processed_data, test_size=test_size, random_state=42)\n","    # Instantiating the DistilBERT tokenizer\n","    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","    # Instantiating the training and testing datasets\n","    train_dataset = EmotionDataset(train_data, tokenizer)\n","    test_dataset = EmotionDataset(test_data, tokenizer)\n","    # Instantiating the training and testing dataloader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","    return train_loader, test_loader\n","\n","# Instantiate and train RoBERTa model\n","train_loader, test_loader = text_processing_pipeline(all_data)\n"]},{"cell_type":"code","execution_count":14,"id":"b06dbb5f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 46/46 [00:11<00:00,  3.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 1.37367684426515\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 1.0513750211052273\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.6481103871179663\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.48690016729676205\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.32836243137717247\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.19519551632844884\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.12099987377777048\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.07444361953631691\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.055243108502548675\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 46/46 [00:11<00:00,  4.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.03646675827305602\n","Accuracy: 0.8066\n","F1-Score: 0.8064\n"]}],"source":["# Instantiating the RoBERTa model\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4) \n","model.to('cuda') # Moving the model to the GPU\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5) # Instantiating the optimizer\n","\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=4).to('cuda') # Instantiating the accuracy metric\n","f1_metric = F1Score(task=\"multiclass\", num_classes=4, average='weighted').to('cuda') # Instantiating the f1 metric\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train() # Setting the model to training mode\n","    total_loss = 0\n","    for batch in tqdm(train_loader):\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        labels = batch['label'].to('cuda')\n","        optimizer.zero_grad() # Resetting gradients\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # Forward pass to get the output probabilities\n","        loss = outputs.loss # Calculating the loss\n","        total_loss += loss.item() # Calculating the total loss\n","        loss.backward() # Back propagation\n","        optimizer.step() # Performing an optimizer step\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'roberta_emotion_detection.pth')\n","\n","model.eval() # Setting the model to evaluation mode\n","# Resetting the accuracy and f1 metrics\n","accuracy_metric.reset() \n","f1_metric.reset()\n","\n","# Evaluate the model\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        labels = batch['label'].to('cuda')\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Forward pass to get the output probabilities\n","        preds = torch.argmax(outputs.logits, dim=1) # Getting the class with the highest probability            \n","        # Update metrics\n","        accuracy_metric.update(preds, labels)\n","        f1_metric.update(preds, labels)\n","\n","# Compute final scores\n","accuracy = accuracy_metric.compute().item()\n","f1 = f1_metric.compute().item()\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
